{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow_transform.saved import input_fn_maker, saved_transform_io\n",
    "from tensorflow_transform.tf_metadata import metadata_io\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "CSV_COLUMNS = 'fare_amount,dayofweek,hourofday,pickuplon,pickuplat,dropofflon,dropofflat,passengers,key'.split(',')\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "KEY_FEATURE_COLUMN = 'key'\n",
    "DEFAULTS = [[0.0], ['Sun'], [0], [-74.0], [40.0], [-74.0], [40.7], [1.0], ['nokey']]\n",
    "\n",
    "# These are the raw input columns, and will be provided for prediction also\n",
    "INPUT_COLUMNS = [\n",
    "    # Define features\n",
    "    tf.feature_column.categorical_column_with_identity('dayofweek', num_buckets = 100 ),  # some large number\n",
    "    tf.feature_column.categorical_column_with_identity('hourofday', num_buckets = 24),\n",
    "\n",
    "    # Numeric columns\n",
    "    tf.feature_column.numeric_column('pickuplon'),\n",
    "    tf.feature_column.numeric_column('pickuplat'),\n",
    "    tf.feature_column.numeric_column('dropofflat'),\n",
    "    tf.feature_column.numeric_column('dropofflon'),\n",
    "    tf.feature_column.numeric_column('passengers'),\n",
    "    \n",
    "    # Engineered features that are created in the input_fn\n",
    "    tf.feature_column.numeric_column('latdiff'),\n",
    "    tf.feature_column.numeric_column('londiff'),\n",
    "    tf.feature_column.numeric_column('euclidean')\n",
    "]\n",
    "\n",
    "# Build the estimator\n",
    "def build_estimator(model_dir, nbuckets, hidden_units):\n",
    "    \"\"\"\n",
    "     Build an estimator starting from INPUT COLUMNS.\n",
    "     These include feature transformations and synthetic features.\n",
    "     The model is a wide-and-deep model.\n",
    "  \"\"\"\n",
    "\n",
    "    # Input columns\n",
    "    (dayofweek, hourofday, latdiff, londiff, euclidean, plon, plat, dlon, dlat, pcount) = INPUT_COLUMNS\n",
    "\n",
    "    # Bucketize the lats & lons\n",
    "    latbuckets = np.linspace(0, 1.0, nbuckets).tolist()\n",
    "    lonbuckets = np.linspace(0, 1.0, nbuckets).tolist()\n",
    "    b_plat = tf.feature_column.bucketized_column(plat, latbuckets)\n",
    "    b_dlat = tf.feature_column.bucketized_column(dlat, latbuckets)\n",
    "    b_plon = tf.feature_column.bucketized_column(plon, lonbuckets)\n",
    "    b_dlon = tf.feature_column.bucketized_column(dlon, lonbuckets)\n",
    "\n",
    "    # Feature cross\n",
    "    ploc = tf.feature_column.crossed_column([b_plat, b_plon], nbuckets * nbuckets)\n",
    "    dloc = tf.feature_column.crossed_column([b_dlat, b_dlon], nbuckets * nbuckets)\n",
    "    pd_pair = tf.feature_column.crossed_column([ploc, dloc], nbuckets ** 4 )\n",
    "    day_hr =  tf.feature_column.crossed_column([dayofweek, hourofday], 24 * 7)\n",
    "\n",
    "    # Wide columns and deep columns.\n",
    "    wide_columns = [\n",
    "        # Feature crosses\n",
    "        dloc, ploc, pd_pair,\n",
    "        day_hr,\n",
    "\n",
    "        # Sparse columns\n",
    "        dayofweek, hourofday,\n",
    "\n",
    "        # Anything with a linear relationship\n",
    "        pcount \n",
    "    ]\n",
    "\n",
    "    deep_columns = [\n",
    "        # Embedding_column to \"group\" together ...\n",
    "        tf.feature_column.embedding_column(pd_pair, 10),\n",
    "        tf.feature_column.embedding_column(day_hr, 10),\n",
    "\n",
    "        # Numeric columns\n",
    "        plat, plon, dlat, dlon,\n",
    "        latdiff, londiff, euclidean\n",
    "    ]\n",
    "    \n",
    "    return tf.estimator.DNNLinearCombinedRegressor(\n",
    "        model_dir = model_dir,\n",
    "        linear_feature_columns = wide_columns,\n",
    "        dnn_feature_columns = deep_columns,\n",
    "        dnn_hidden_units = hidden_units)\n",
    "\n",
    "# Create serving input function to be able to serve predictions\n",
    "def make_serving_input_fn_for_base64_json(args):\n",
    "    raw_metadata = metadata_io.read_metadata(\n",
    "        os.path.join(args['metadata_path'], 'rawdata_metadata'))\n",
    "    transform_savedmodel_dir = (\n",
    "        os.path.join(args['metadata_path'], 'transform_fn'))\n",
    "    return input_fn_maker.build_parsing_transforming_serving_input_receiver_fn(\n",
    "      raw_metadata,\n",
    "      transform_savedmodel_dir,\n",
    "      exclude_raw_keys = [LABEL_COLUMN])\n",
    "\n",
    "def make_serving_input_fn(args):\n",
    "  transform_savedmodel_dir = (\n",
    "        os.path.join(args['metadata_path'], 'transform_fn'))\n",
    "\n",
    "  def _input_fn():\n",
    "    # placeholders for all the raw inputs\n",
    "    feature_placeholders = {\n",
    "      column_name: tf.placeholder(tf.float32, [None]) for column_name in 'pickuplon,pickuplat,dropofflat,dropofflon'.split(',')\n",
    "    }\n",
    "    feature_placeholders['passengers'] = tf.placeholder(tf.int64, [None])\n",
    "    feature_placeholders['dayofweek'] = tf.placeholder(tf.string, [None])\n",
    "    feature_placeholders['hourofday'] = tf.placeholder(tf.int64, [None])\n",
    "    feature_placeholders['key'] = tf.placeholder(tf.string, [None])\n",
    "\n",
    "    # transform using the saved model in transform_fn\n",
    "    _, features = saved_transform_io.partially_apply_saved_transform(\n",
    "      transform_savedmodel_dir,\n",
    "      feature_placeholders\n",
    "    )\n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n",
    "\n",
    "  return _input_fn\n",
    "\n",
    "# Create input function to load data into datasets\n",
    "def read_dataset(args, mode):\n",
    "    batch_size = args['train_batch_size']\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        input_paths = args['train_data_paths']\n",
    "    else:\n",
    "        input_paths = args['eval_data_paths']\n",
    "\n",
    "    transformed_metadata = metadata_io.read_metadata(\n",
    "              os.path.join(args['metadata_path'], 'transformed_metadata'))\n",
    "\n",
    "    return input_fn_maker.build_training_input_fn(\n",
    "          metadata = transformed_metadata,\n",
    "          file_pattern = (\n",
    "              input_paths[0] if len(input_paths) == 1 else input_paths),\n",
    "          training_batch_size = batch_size,\n",
    "          label_keys = [LABEL_COLUMN],\n",
    "          reader = gzip_reader_fn,\n",
    "          key_feature_name = KEY_FEATURE_COLUMN,\n",
    "          randomize_input = (mode != tf.estimator.ModeKeys.EVAL),\n",
    "          num_epochs = (1 if mode == tf.estimator.ModeKeys.EVAL else None)) \n",
    "\n",
    "# Create estimator train and evaluate function\n",
    "def train_and_evaluate(args):\n",
    "    tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "    estimator = build_estimator(args['output_dir'], args['nbuckets'], args['hidden_units'].split(' '))\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = read_dataset(args, tf.estimator.ModeKeys.TRAIN),\n",
    "        max_steps = args['train_steps'])\n",
    "    exporter = tf.estimator.LatestExporter(\n",
    "        'exporter', make_serving_input_fn(args))\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = read_dataset(args, tf.estimator.ModeKeys.EVAL),\n",
    "        steps = None,\n",
    "        exporters = exporter)\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "\n",
    "# If we want to use TFRecords instead of CSV\n",
    "def gzip_reader_fn():\n",
    "    return tf.TFRecordReader(options=tf.python_io.TFRecordOptions(\n",
    "            compression_type = tf.python_io.TFRecordCompressionType.GZIP))\n",
    "\n",
    "def get_eval_metrics():\n",
    "    return {\n",
    "        'rmse': tflearn.MetricSpec(metric_fn=metrics.streaming_root_mean_squared_error),\n",
    "        'training/hptuning/metric': tflearn.MetricSpec(metric_fn=metrics.streaming_root_mean_squared_error),\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
