{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "CSV_COLUMNS = 'fare_amount,dayofweek,hourofday,pickuplon,pickuplat,dropofflon,dropofflat,passengers,key'.split(',')\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "KEY_FEATURE_COLUMN = 'key'\n",
    "DEFAULTS = [[0.0], ['Sun'], [0], [-74.0], [40.0], [-74.0], [40.7], [1.0], ['nokey']]\n",
    "\n",
    "# These are the raw input columns, and will be provided for prediction also\n",
    "INPUT_COLUMNS = [\n",
    "    # Define features\n",
    "    tf.feature_column.categorical_column_with_vocabulary_list('dayofweek', vocabulary_list = ['Sun', 'Mon', 'Tues', 'Wed', 'Thu', 'Fri', 'Sat']),\n",
    "    tf.feature_column.categorical_column_with_identity('hourofday', num_buckets = 24),\n",
    "\n",
    "    # Numeric columns\n",
    "    tf.feature_column.numeric_column('pickuplat'),\n",
    "    tf.feature_column.numeric_column('pickuplon'),\n",
    "    tf.feature_column.numeric_column('dropofflat'),\n",
    "    tf.feature_column.numeric_column('dropofflon'),\n",
    "    tf.feature_column.numeric_column('passengers'),\n",
    "    \n",
    "    # Engineered features that are created in the input_fn\n",
    "    tf.feature_column.numeric_column('latdiff'),\n",
    "    tf.feature_column.numeric_column('londiff'),\n",
    "    tf.feature_column.numeric_column('euclidean')\n",
    "]\n",
    "\n",
    "# Build the estimator\n",
    "def build_estimator(model_dir, nbuckets, hidden_units):\n",
    "    \"\"\"\n",
    "     Build an estimator starting from INPUT COLUMNS.\n",
    "     These include feature transformations and synthetic features.\n",
    "     The model is a wide-and-deep model.\n",
    "  \"\"\"\n",
    "\n",
    "    # Input columns\n",
    "    (dayofweek, hourofday, plat, plon, dlat, dlon, pcount, latdiff, londiff, euclidean) = INPUT_COLUMNS\n",
    "\n",
    "    # Bucketize the lats & lons\n",
    "    latbuckets = np.linspace(38.0, 42.0, nbuckets).tolist()\n",
    "    lonbuckets = np.linspace(-76.0, -72.0, nbuckets).tolist()\n",
    "    b_plat = tf.feature_column.bucketized_column(plat, latbuckets)\n",
    "    b_dlat = tf.feature_column.bucketized_column(dlat, latbuckets)\n",
    "    b_plon = tf.feature_column.bucketized_column(plon, lonbuckets)\n",
    "    b_dlon = tf.feature_column.bucketized_column(dlon, lonbuckets)\n",
    "\n",
    "    # Feature cross\n",
    "    ploc = tf.feature_column.crossed_column([b_plat, b_plon], nbuckets * nbuckets)\n",
    "    dloc = tf.feature_column.crossed_column([b_dlat, b_dlon], nbuckets * nbuckets)\n",
    "    pd_pair = tf.feature_column.crossed_column([ploc, dloc], nbuckets ** 4 )\n",
    "    day_hr =  tf.feature_column.crossed_column([dayofweek, hourofday], 24 * 7)\n",
    "\n",
    "    # Wide columns and deep columns.\n",
    "    wide_columns = [\n",
    "        # Feature crosses\n",
    "        dloc, ploc, pd_pair,\n",
    "        day_hr,\n",
    "\n",
    "        # Sparse columns\n",
    "        dayofweek, hourofday,\n",
    "\n",
    "        # Anything with a linear relationship\n",
    "        pcount \n",
    "    ]\n",
    "\n",
    "    deep_columns = [\n",
    "        # Embedding_column to \"group\" together ...\n",
    "        tf.feature_column.embedding_column(pd_pair, 10),\n",
    "        tf.feature_column.embedding_column(day_hr, 10),\n",
    "\n",
    "        # Numeric columns\n",
    "        plat, plon, dlat, dlon,\n",
    "        latdiff, londiff, euclidean\n",
    "    ]\n",
    "    \n",
    "    ## setting the checkpoint interval to be much lower for this task\n",
    "    run_config = tf.estimator.RunConfig(save_checkpoints_secs = 30, \n",
    "                                        keep_checkpoint_max = 3)\n",
    "    estimator = tf.estimator.DNNLinearCombinedRegressor(\n",
    "        model_dir = model_dir,\n",
    "        linear_feature_columns = wide_columns,\n",
    "        dnn_feature_columns = deep_columns,\n",
    "        dnn_hidden_units = hidden_units,\n",
    "        config = run_config)\n",
    "\n",
    "    # add extra evaluation metric for hyperparameter tuning\n",
    "    estimator = tf.contrib.estimator.add_metrics(estimator, add_eval_metrics)\n",
    "    return estimator\n",
    "\n",
    "# Create feature engineering function that will be used in the input and serving input functions\n",
    "def add_engineered(features):\n",
    "    # this is how you can do feature engineering in TensorFlow\n",
    "    lat1 = features['pickuplat']\n",
    "    lat2 = features['dropofflat']\n",
    "    lon1 = features['pickuplon']\n",
    "    lon2 = features['dropofflon']\n",
    "    latdiff = (lat1 - lat2)\n",
    "    londiff = (lon1 - lon2)\n",
    "    \n",
    "    # set features for distance with sign that indicates direction\n",
    "    features['latdiff'] = latdiff\n",
    "    features['londiff'] = londiff\n",
    "    dist = tf.sqrt(latdiff * latdiff + londiff * londiff)\n",
    "    features['euclidean'] = dist\n",
    "    return features\n",
    "\n",
    "# Create serving input function to be able to serve predictions\n",
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        # All the real-valued columns\n",
    "        column.name: tf.placeholder(tf.float32, [None]) for column in INPUT_COLUMNS[2:7]\n",
    "    }\n",
    "    feature_placeholders['dayofweek'] = tf.placeholder(tf.string, [None])\n",
    "    feature_placeholders['hourofday'] = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    features = add_engineered(feature_placeholders.copy())\n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n",
    "\n",
    "# Create input function to load data into datasets\n",
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "    def _input_fn():\n",
    "        def decode_csv(value_column):\n",
    "            columns = tf.decode_csv(value_column, record_defaults = DEFAULTS)\n",
    "            features = dict(zip(CSV_COLUMNS, columns))\n",
    "            label = features.pop(LABEL_COLUMN)\n",
    "            return add_engineered(features), label\n",
    "        \n",
    "        # Create list of files that match pattern\n",
    "        file_list = tf.gfile.Glob(filename)\n",
    "\n",
    "        # Create dataset from file list\n",
    "        dataset = tf.data.TextLineDataset(file_list).map(decode_csv)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "        batch_features, batch_labels = dataset.make_one_shot_iterator().get_next()\n",
    "        return batch_features, batch_labels\n",
    "    return _input_fn\n",
    "\n",
    "# Create estimator train and evaluate function\n",
    "def train_and_evaluate(args):\n",
    "    tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "    estimator = build_estimator(args['output_dir'], args['nbuckets'], args['hidden_units'].split(' '))\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = read_dataset(\n",
    "            filename = args['train_data_paths'],\n",
    "            mode = tf.estimator.ModeKeys.TRAIN,\n",
    "            batch_size = args['train_batch_size']),\n",
    "        max_steps = args['train_steps'])\n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = read_dataset(\n",
    "            filename = args['eval_data_paths'],\n",
    "            mode = tf.estimator.ModeKeys.EVAL,\n",
    "            batch_size = args['eval_batch_size']),\n",
    "        steps = 100,\n",
    "        exporters = exporter)\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "\n",
    "# If we want to use TFRecords instead of CSV\n",
    "def gzip_reader_fn():\n",
    "    return tf.TFRecordReader(options=tf.python_io.TFRecordOptions(\n",
    "            compression_type = tf.python_io.TFRecordCompressionType.GZIP))\n",
    "\n",
    "def generate_tfrecord_input_fn(data_paths, num_epochs = None, batch_size = 512, mode = tf.estimator.ModeKeys.TRAIN):\n",
    "    def get_input_features():\n",
    "        # Read the tfrecords. Same input schema as in preprocess\n",
    "        input_schema = {}\n",
    "        if mode != tf.estimator.ModeKeys.INFER:\n",
    "            input_schema[LABEL_COLUMN] = tf.FixedLenFeature(shape = [1], dtype = tf.float32, default_value = 0.0)\n",
    "        for name in ['dayofweek', 'key']:\n",
    "            input_schema[name] = tf.FixedLenFeature(shape = [1], dtype = tf.string, default_value = 'null')\n",
    "        for name in ['hourofday']:\n",
    "            input_schema[name] = tf.FixedLenFeature(shape = [1], dtype = tf.int64, default_value = 0)\n",
    "        for name in SCALE_COLUMNS:\n",
    "            input_schema[name] = tf.FixedLenFeature(shape = [1], dtype = tf.float32, default_value = 0.0)\n",
    "\n",
    "        # How? \n",
    "        keys, features = tf.contrib.learn.io.read_keyed_batch_features(\n",
    "            data_paths[0] if len(data_paths) == 1 else data_paths,\n",
    "            batch_size,\n",
    "            input_schema,\n",
    "            reader = gzip_reader_fn,\n",
    "            reader_num_threads = 4,\n",
    "            queue_capacity = batch_size * 2,\n",
    "            randomize_input = (mode != tf.estimator.ModeKeys.EVAL),\n",
    "            num_epochs = (1 if mode == tf.estimator.ModeKeys.EVAL else num_epochs))\n",
    "        target = features.pop(LABEL_COLUMN)\n",
    "        features[KEY_FEATURE_COLUMN] = keys\n",
    "        return add_engineered(features), target\n",
    "\n",
    "    # Return a function to input the features into the model from a data path.\n",
    "    return get_input_features\n",
    "\n",
    "def add_eval_metrics(labels, predictions):\n",
    "    pred_values = predictions['predictions']\n",
    "    return {\n",
    "        'rmse': tf.metrics.root_mean_squared_error(labels, pred_values)\n",
    "    }\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
